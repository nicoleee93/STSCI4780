{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab05 Exercises\n",
    "\n",
    "These exercises are in lieu of an assignment (because of the February break). Some of these exercises will be part of future assignments. Solutions won't be provided, but we'll be happy to help you work on these problems in office hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Change of variables from probability to odds\n",
    "\n",
    "Consider inference problems with Bernoulli outcomes with success probability $\\alpha$ (e.g., estimating $\\alpha$ from binary sequence data using a Bernoulli process model, or from count data using a binomial distribution model). Suppose the prior for $\\alpha$ is some specified function $f(\\alpha)$. Define the single-trial odds in favor of success as\n",
    "\\begin{align}\n",
    "\\omega &\\equiv \\frac{\\alpha}{1-\\alpha}\\\\\n",
    "  &\\equiv \\Omega(\\alpha).\n",
    "\\end{align}\n",
    "\n",
    "> * What is the inverse mapping, $A(\\omega)$, that gives the value of $\\alpha$ corresponding to a given value of $\\omega$?\n",
    "> * What PDF for $\\omega$, $g(\\omega)$, corresponds to a given PDF for $\\alpha$, $f(\\alpha)$? Be sure that $g(\\omega)$ is expressed in terms of $\\omega$, i.e., $\\alpha$ should not appear. Compute any derivatives that appear in the formula.\n",
    "> * A uniform prior PDF for $\\alpha$ corresponds to $f(\\alpha) = 1$ (this is normalized over $[0,1]$). What prior for $\\omega$ corresponds to this uniform prior for $\\alpha$?\n",
    "> * Verify that the PDF for $\\omega$ you just derived is normalized.\n",
    "> * The uniform prior for $\\alpha$ assigns probability $1/2$ for $\\alpha$ being in the interval $[0,1/2]$. Identify the corresponding interval for $\\omega$, and show that the PDF for $\\omega$ you just computed assigns probability $1/2$ to that interval.\n",
    "\n",
    "For this exercise you'll need to use the PDF transformation rule described in Lec10 (\"change of variables\"). For online presentations on this topic, see:\n",
    "* [Lesson 22: Functions of One Random Variable | Penn State U. STAT 414](https://online.stat.psu.edu/stat414/lesson/22) (particularly section 22.2)\n",
    "* [Probability/Transformation of Probability Densities – Wikibooks](https://en.wikibooks.org/wiki/Probability/Transformation_of_Probability_Densities#Function_of_a_Random_Variable_(n=1,_m=1)) (this presents alternative derivations to the one in Lec10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Cauchy distribution\n",
    "\n",
    "*We currently plan on including this problem in Assignment04.*\n",
    "\n",
    "The *Cauchy distribution* is a special case of the Student's $t$ distribution covered in Lec07; it corresponds to Student's $t$ with degrees of freedom parameter $\\nu=1$. It has an undefined mean and an infinite variance.  It is troublesome to work with in frequentist statistics.  Even it's maximum likelihood estimator has complicated sampling properties that pose not just computational challenges, but conceptual ones (the best frequentist methods require adopting something called the *conditional frequentist approach*, related to the likelihood principle).  In Bayesian inference, it poses no conceptual difficulties, but it must be handled numerically (which you'll do in a future assignment).\n",
    "\n",
    "The Cauchy distribution is known in physics as the *Lorentzian distribution*, where in certain circumstances it describes the profile of spectral lines, and the shape of peaks in mass spectra measured by particle accelerator experiments.  It also appears in problems where the ratio of two quantities with normal errors is of interest—when the quantities are uncorrelated with zero mean, the PDF for the ratio is a Cauchy distribution.  As noted above, the Student's $t$ distribution with 1 degree of freedom is a Cauchy distribution.  It also arises in geometric inference problems, as you will see in this exercise.\n",
    "\n",
    "You can find basic information about the Cauchy distribution [on Wikipedia](http://en.wikipedia.org/wiki/Cauchy_distribution) and in the [NIST Engineering Statistics Handbook](http://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm).\n",
    "\n",
    "**Exercise:** A rod of radioactive material is a distance $d$ behind a barrier (e.g., this could be a fuel rod inside a nuclear reactor). A narrow sensor strip is on the barrier, oriented orthogonal to the rod (and not near either of its ends). The rod is at an unknown position, $x_0$, along the sensor.  The sensor records the locations of $N$ gamma rays emitted by the rod, denoted $x_i$ (for $i=1$ to $N$).  We'll assume that $d$ is small compared to the length of the sensor, so that the sensor may be considered essentially infinite in length.\n",
    "\n",
    "The geometry is shown in the following figure, oriented looking down along the rod (shown as a large dot).\n",
    "\n",
    "<img src=\"CauchyGeometry.png\"/>\n",
    "\n",
    "> Assume the rod emits gamma rays isotropically (i.e., with a uniform distribution in the angle $\\theta$; consider only angles corresponding to potentially detectable gamma rays, i.e., with angles corresponding to directions in the lower half-plane in the figure).  Show that the PDF for the detected location of a single gamma ray, $x$, is a Cauchy distribution with location parameter $x_0$ and scale parameter $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predicting a Bernoulli outcome\n",
    "\n",
    "Suppose you have a parametric model for some available data, $D$; denote the parameters by $\\theta$ (as usual!). The posterior PDF for $\\theta$ is\n",
    "$$\n",
    "p(\\theta|D,\\mathcal{C}) = \\frac{p(\\theta|\\mathcal{C})\\, p(D|\\theta,\\mathcal{C})}{p(D|\\mathcal{C})}.\n",
    "$$\n",
    "Now suppose you'd like to predict future data, $D'$. From a Bayesian perspective, you would do this by computing the **posterior predictive distribution**, $p(D'|D,\\mathcal{C})$, discussed briefly (on just one slide!) in Lec09.  Here we'll recap the lecture material, and then you'll work through a specific example.\n",
    "\n",
    "It's probably not immediately clear how to compute $p(D'|D,\\mathcal{C})$. On the other hand, since we are working in the context of a parametric model, if we knew the value of the parameters, $\\theta$, then we'd be able to compute a predictive distribution: it would just be the (parametric) sampling distribution for the *new* data, $p(D'|\\theta,\\mathcal{C})$. This suggests using the LTP (in its \"wishful thinking\" mode!) to compute the posterior predictive distribution, as follows:\n",
    "\\begin{align}\n",
    "p(D'|D)\n",
    "  &= \\int d\\theta\\, p(D',\\theta|D) \\qquad ||\\,\\mathcal{C}\\\\\n",
    "  &= \\int d\\theta\\, p(\\theta|D)\\, p(D'|\\theta,D)\\\\\n",
    "  &= \\int d\\theta\\, p(\\theta|D)\\, p(D'|\\theta).\n",
    "\\end{align}\n",
    "For the last line, we took advantage of the fact that knowing $\\theta$ determines the sampling distribution for any data (this is what we mean by a \"parametric model\"), so conditioning on $D$ doesn't affect $p(D'|\\theta,D)$.\n",
    "\n",
    "On that last line, the first factor is the posterior PDF for $\\theta$ based on the available data, $D$.  The second factor is the predictive probability for $D'$ if we knew $\\theta$ (i.e., the sampling distribution for $D'$). So this equation tells us the predictive distribution is an average of the parametric sampling distributions—averaged using the posterior for $\\theta$ as the averaging weight.\n",
    "\n",
    "Apply this to a simple Bernoulli trials example, using binomial sampling distributions: In the IID Bernoulli trial setting, suppose you see $n$ heads in $N$ flips of a coin.  You plan to do $M$ more flips.  What's the predictive probability for seeing $m$ heads among those new flips?\n",
    "\n",
    "> * Denote the probability for heads in a single trial by $\\alpha$ (as usual!). If you've seen $n$ heads in $N$ trials, what is the posterior for $\\alpha$? (Just write this down from your notes; you needn't derive it anew.) In the context of the posterior predictive distribution above, this PDF is $p(\\theta|D)$ (with $\\theta\\rightarrow\\alpha$, and $D\\rightarrow n$).\n",
    "> * If you knew $\\alpha$, what would be the probability for seeing $m$ heads in $M$ new trials? In the context of the posterior predictive distribution above, this (simple!) PMF is $p(D'|\\theta)$.\n",
    "> * Compute the posterior predictive probability for seeing $m$ heads in $M$ new trials, given that you've seen $n$ heads in $N$ past trials, $p(m|n)$ (the numbers of trials are part of the contextual information, dropped here).\n",
    "> * The resulting formula is not very illuminating.  But the special case of *one new trial* is revealing.  What is $p(m=1|n)$, when there is going to be just $M=1$ new flip?\n",
    "> * What does that result say when $N$ and $n$ are large?\n",
    "> * The formula for $p(m=1|n)$ (when $M=1$) should be a simple ratio that resembles $n/N$, but with somewhat curious adjustments to the numerator and denominator. To get a sense of why they are there, find the value for $p(m=1|n)$ when there is *no prior data*, i.e., $N=0$, $n=0$.\n",
    "\n",
    "*Hint:* Keep the beta function formula handy. You are reproducing a very famous calculation done by Laplace; the result is known as *Laplace's rule of succession*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
