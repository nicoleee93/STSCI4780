{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "\n",
    "**Due:** Thursday, 2022-02-24, 11:59 PM, as a Jupyter notebook submitted via your repo in the course GitHub organization (see the detailed instructions in our LabResources repo).  Edit the provided Solutions03 notebook with your solutions.  All subproblems (or problems with no subproblems) are worth 1 point unless otherwise noted (grading will give fractional points as appropriate).  Be sure to include the required Python module alongside your solutions notebook in your repo.\n",
    "\n",
    "In derivations, use a bit of text to explain your steps, but you needn't write an essay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The \"Or\" rule\n",
    "\n",
    "Recall the disjunction (logical \"or\") rule:\n",
    "$$\n",
    "P(A\\lor B) = P(A) + P(B) - P(A\\land B) \\qquad ||\\;\\mathcal{C}.\n",
    "$$\n",
    "In the majority of applications, the set of propositions of interest (about either parameters or data) typically comprise an *exhaustive, mutually exclusive* set.  (In the book *Think Bayes*, author Allen Downey uses the term *suite* to denote such a set of propositions; I like that!)  For mutually exclusive propositions, $P(A\\land B) = 0$. For example, if a parameter has one value, it can't simultaneously take on another value; similarly for the value of a datum. In this case, the \"or\" rule simplifies (dropping the $\\mathcal{C}$ henceforth):\n",
    "$$\n",
    "P(A\\lor B) = P(A) + P(B).\n",
    "$$\n",
    "It's simpler than the full \"or\" rule, though we've only dropped 1 term.\n",
    "\n",
    "When we derived the LTP, we used this simpler \"or\" rule, but extended to an arbitrary number of propositions, $B_i$, comprising a suite. The purpose of this exercise is to help you appreciate the impact of working with a mutually exclusive set of alternatives.  It simplifies things more than you may have realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=1",
     "points=2"
    ]
   },
   "source": [
    "### Problem 1 (2 points):\n",
    "\n",
    "> Use the \"or\" and \"and\" rules to find an expression for $P(A\\lor B \\lor C)$, *not* assuming that $A$, $B$, and $C$ are mutually exclusive. Seek a result using only probabilities for the individual propositions, and \"and\" combinations of them. (You can use a comma instead of $\\land$ for \"and\" if you wish.)  You may use associativity of \"or\" and \"and;\" that is, $A\\lor (B \\lor C) \\equiv (A\\lor B) \\lor C$, and $A\\land (B \\land C) \\equiv (A\\land B) \\land C$.\n",
    "\n",
    "> Do this in two steps:\n",
    "\n",
    "> * Copy the following incomplete truth tables into your solution cell and complete them. For each table, you should find that the two \"starred\" columns (with a leading asterisk—that's not a logic symbol!) have the same truth values in every row, indicating they are logically equivalent. These tables thus establish two *rules of replacement*: where you see one of these formulas, you may substitute the other.  MathJax may make the headings hard to read, so to clarify: the first table aims to show $X\\land Y\\land X \\equiv X\\land Y$ (so you can drop repeated symbols in a multiple \"and\"), and the second table aims to show a kind of distributive rule: $X\\land (Y\\lor Z) \\equiv (X\\land Y)\\lor (X\\land Z)$.\n",
    "\n",
    "> *Hint*: You may find it easier to work on the tables using the [Markdown Tables generator - TablesGenerator.com](https://www.tablesgenerator.com/markdown_tables) web page. Note that you can copy the Markdown for a table in a notebook to your clipboard, and load it into the web page using the `File->Paste...` command on the web page. Then you can revise it there, and copy the result back.\n",
    "\n",
    "| X | Y | * X $\\land$ Y | * (X $\\land$ Y) $\\land$ X |\n",
    "|---|-----|------------|----------------------|\n",
    "| 0   | 0   |            |                      |\n",
    "| 0   | 1   |            |                      |\n",
    "| 1   | 0   |            |                      |\n",
    "| 1   | 1   |            |                      |\n",
    "\n",
    "\n",
    "| X | Y | Z | Y $\\lor$ Z | * X $\\land$ (Y $\\lor$ Z)  | X $\\land$ Y | X $\\land$ Z | * (X $\\land$ Y) $\\lor$ (X $\\land$ Z) |\n",
    "|---|-----|-----|-----------|---------------------|------------|------------|-----------------------------|\n",
    "| 0   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 1   |           |                     |            |            |                             |\n",
    "\n",
    "> * With those replacement rules in hand, proceed with deriving the 3-proposition \"or\" rule.\n",
    "\n",
    "> *Hint*: You might guess from the 2-proposition \"or\" rule that the answer is $P(A\\lor B \\lor C) = P(A) + P(B) + P(C) - P(A\\land B \\land C)$. But that's not right. It's quite a bit more complicated (which is why having mutually exclusive alternatives is a great help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discovery chains\n",
    "\n",
    "Suppose you try to learn whether a coin flipper is biased by doing two experiments: you first count 39 heads in 100 flips, and then 55 heads in another 100 flips. From a Bayesian perspective, you could analyze these data by first finding a posterior using the 39/100 data, and then using that posterior as a prior for computing a final posterior considering in addition the 55/100 data. But you might also consider pooling the data, and just doing one calculation, computing a posterior based on seeing 94 heads in 200 flips. You might intuitively expect the two calculations will agree—and they do! This problem explores this kind of agreement.\n",
    "\n",
    "The parameter estimation problems addressed in lectures 5 through 7 involve the use of *conjugate families*: prior/likelihood pairs such that if the prior is a distribution in a particular family, and the likelihood function is built from a sampling distribution in the partner family, then the posterior distribution is another distribution from the prior's family.\n",
    "\n",
    "A handy feature of conjugate families is that inferences can be easily chained, with the posterior from inference based on an initially considered dataset, $D_1$, serving as the prior for analysis of a subsequently considered dataset, $D_2$, and producing an updated posterior that can be found by simple manipulation of the parameters in the prior family.\n",
    "\n",
    "For example, in Lec05 we considered inference of a binary outcome probability, $\\alpha$, with binomial data comprising $n$ successes in $N$ trials.  We found the posterior PDF for $\\alpha$, based on a beta PDF prior with parameters $(a,b)$:\n",
    "\\begin{align}\n",
    "p(\\alpha|n,M') \n",
    "  &\\propto \\mathop{\\mathrm{Beta}}(\\alpha|a,b) \\times \\mathop{\\mathrm{Binom}}(n|\\alpha,N)\\\\\n",
    "  &\\propto \\alpha^{a-1} (1-\\alpha)^{b-1} \\times \\alpha^{n} (1-\\alpha)^{N-n}\\\\\n",
    "  &\\propto \\alpha^{n+a-1} (1-\\alpha)^{N-n+b-1}.\n",
    "\\end{align}\n",
    "That is, the posterior is $\\mathop{\\mathrm{Beta}}(\\alpha|n+a,N-n+b)$. So schematically, the update rule, telling us how a beta prior is modified into a beta posterior, is:\n",
    "$$\n",
    "\\boxed{(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b).}\n",
    "$$\n",
    "Thus if we get additional data composed of $n'$ successes in $N'$ new trials, the final posterior PDF will again be a beta PDF, with parameters found by chaining/iterating application of the boxed update rule:\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b) \\quad\\Rightarrow\\quad (n+n'+a,\\; N+N'-n-n'+b).\n",
    "$$\n",
    "\n",
    "A simple but interesting consequence of this update rule is that we get the same overall posterior if we *pool the data*, i.e., consider a *single* binomial dataset composed of $m=n+n'$ successes in $M=N+N'$ trials.  Applying the single-stage update rule to this pooled data gives\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (m+a,\\; M-m+b) \\quad=\\quad (n+n'+a,\\; N+N'-n-n'+b),\n",
    "$$\n",
    "the same result as we found from chaining the inferences.\n",
    "\n",
    "This result is nearly trivial because, in the binomial IID sampling setting, the probability for the 2nd dataset is independent of the outcome comprising the 1st dataset, if we are given $\\alpha$ (as we are in a likelihood function—the probability for the data *given* the values of any parameters). Independence implies that the likelihood function based on the pooled data is just the product of the likelihood functions considering each dataset separately, and consistency follows easily from this.\n",
    "\n",
    "In fact, the consistency of the chained and pooled inferences is a more general result that *does not require the two datasets to have sampling distributions that are conditionally independent* (i.e., independent given the parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=2",
     "points=2"
    ]
   },
   "source": [
    "### Problem 2 (2 points):\n",
    "\n",
    "> Prove the *general* consistency of chained and joint inferences based on using two datasets to estimate a parameter, $\\theta$ (general in the sense of not assuming conditional independence).\n",
    "\n",
    "> 1. Use Bayes's theorem to write down the posterior PDF for $\\theta$ based on data $D_1$.\n",
    "> 2. Use the posterior from (1) as the prior for inference of $\\theta$ additionally considering new data, $D_2$, using Bayes's theorem to compute an overall posterior PDF for $\\theta$, $p(\\theta|D_1,D_2,\\mathcal{C})$. *Do not assume that the joint sampling distribution for $(D_1,D_2)$ factors*:\n",
    "$$\n",
    "p(D_1,D_2|\\theta) \\ne p(D_1|\\theta)\\times p(D_2|\\theta). \\qquad ||\\; \\mathcal{C}\n",
    "$$\n",
    "> 3. Now suppose you start with the same initial prior used in (1), but consider the two datasets together. Compute the posterior $p(\\theta|D_1,D_2,\\mathcal{C})$ in a single step, considering $(D_1,D_2)$ as a single, pooled dataset.\n",
    "> 4. Show that the results of (2) and (3) are equal (i.e., use the rules of probability theory to show that one result equals the other).\n",
    "\n",
    "> For convenience, you may drop $\\mathcal{C}$ from the notation, since the same contextual information is being used throughout.\n",
    "\n",
    "> *Hint:* You shouldn't have to write out any marginal likelihoods (i.e., writing them as integrals of prior times likelihood) in order to prove consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The normal-normal conjugate model\n",
    "\n",
    "In Lec07 you were *told* that a normal prior is a conjugate prior for the likelihood function for the mean of a set of data modeled with IID normal additive noise, and you were given equations for how the posterior mode (or mean) and the posterior width (standard deviation) depend on both the data and the prior mean and width (see Lec07, slide 11).  For this problem, you'll *derive* those results, and duplicate a plot from the lecture, using both analytical and numerical approaches.\n",
    "\n",
    "For the derivation, you'll need to a \"completing the square\" calculation, a bit like the one done in Lec07 to show that the IID normal sampling distribution has two sufficient statistics, the sample mean, $\\bar{d}$, and the root-mean-square residual, $r$ (using notation from the lecture). Here's a quick refresher about completing the square.\n",
    "\n",
    "A 2nd-degree polynomial function,\n",
    "$$\n",
    "f(x) = A_2 x^2 + A_1 x + A_0,\n",
    "$$\n",
    "describes a parabolic curve (concave up if $A_2$ is positive, and concave down if it's negative).  A simpler way to parameterize such a curve uses the location of the extremum (minimum or maximum), $x_0$, and the height of the curve at that point, $h$:\n",
    "$$\n",
    "f(x) = a(x - x_0)^2 + h.\n",
    "\\label{f-sqr}\n",
    "$$\n",
    "In this parameterization, $a$ specifies the width of the curve (whether it's a broad or narrow parabola).\n",
    "Completing the square involves manipulating the first expression to identify $(a, x_0, h)$ in the second expression.\n",
    "\n",
    "To see how this could be done, start by expanding the squared term in the second expression:\n",
    "$$\n",
    "f(x) = a(x^2 - 2x_0 x + x_0^2) + h.\n",
    "$$\n",
    "Note that the location parameter, $x_0$, can be identified as the multiplier of the linear term, divided by the factor $(-2a)$ (with that $a$ corresponding to the factor outside the parentheses).  This observation motivates the following procedure to get from the 1st expression for $f(x)$ to the second:\n",
    "\n",
    "Start by rewriting the polynomial in this form:\n",
    "$$\n",
    "f(x) = a x^2 -2bx + c.\n",
    "$$\n",
    "This corresponds to setting $a=A_2$, $b=-A_1/2$, and $c=A_0$. In practice, when doing \"completing the square\" with quadratic polynomials arising in Gaussians, we'll already have the polynomial in this form.\n",
    "\n",
    "Next, group together the $x^2$ and $x$ terms, factoring out $a$:\n",
    "$$\n",
    "f(x) = a \\left( x^2 -2\\frac{b}{a}x\\right) + c.\n",
    "$$\n",
    "This identifies $x_0$ as $b/a$.  We can get both the $x^2$ term and the linear cross term by squaring $(x - x_0)$, but that introduces an extra constant term that doesn't appear in $f(x)$ (the last term here):\n",
    "$$\n",
    "a\\left( x - \\frac{b}{a}\\right)^2 = a x^2 - 2bx + \\frac{b^2}{a}.\n",
    "$$\n",
    "So for the final step, we subtract off that extra term:\n",
    "$$\n",
    "f(x) = a\\left(x - \\frac{b}{a}\\right)^2 - \\frac{b^2}{a} + c.\n",
    "$$\n",
    "We now have $f(x)$ in the form of a \"perfect square\" plus a constant, identifying the parabola's parameters.  For Gaussians, $x_0$ is the location parameter (mean), and $a$ corresponds to the inverse variance (also called the *precision*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.1",
     "points=2"
    ]
   },
   "source": [
    "### Problem 3.1 (2 points):\n",
    "\n",
    "> Use *completing the square* to derive the equations for the posterior mean, $\\tilde{\\mu}$, and the posterior width (standard deviation), $\\tilde{w}$.\n",
    "\n",
    "> ***Hint 1:*** As noted in lecture, the algebra for manipulating Gaussians is often bit easier if you work in terms of the precision, $\\tau \\equiv 1/\\sigma^2$, rather than the standard deviation, $\\sigma$.  So consider working in terms of the prior precision, $\\tau_0 \\equiv 1/w_0^2$, the precision for the likelihood function, $\\tau \\equiv 1/w^2$, and the posterior precision, $\\tilde{\\tau} \\equiv 1/\\tilde{w}^2$.  Convert your expressions for the posterior mean and precision into formulas involving the widths at the end.\n",
    "\n",
    "> ***Hint 2:*** To identify the location and width parameters, you don't care about the constant terms arising when you complete the square (they merely adjust the normalization constant of the final Gaussian).  So feel free to gather uninteresting constants in an unspecified constant, $C$, as you work.  This will save a small bit of algebraic effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.2",
     "points=2"
    ]
   },
   "source": [
    "### Problem 3.2 (2 points):\n",
    "\n",
    "> Create and demonstrate a Python function that can produce plots like those shown on slide 14 of Lec07, showing the prior PDF, likelihood function, and posterior PDF for the normal-normal problem.  Do not implement Bayes's theorem directly (i.e., multiplying prior times likelihood function and normalizing)—you'll do that for Problem 3.3.  Here simply use the analytical results given in the lecture.  (You need not match the lecture figure exactly in terms of choices like color choices or the scale of the likelihood function.)\n",
    "\n",
    "> Write the function in a Python module, for importing into your solution notebook.  A template module is provided as `norm_norm.py`.  Write your code in that file (you can test/debug either in your notebook, or by running it at the command line with `ipython`).  Import the module into your notebook and use the imported functions to produce the requested plots in your notebook.\n",
    "\n",
    "> * Use the function signature `plot_norm_norm1(mus, mu0, w0, dbar, w)`, with the five arguments denoting a vector of $\\mu$ values for computing/plotting, the prior mean, prior width, sample mean, and likelihood width (i.e., the value of $\\sigma/\\sqrt{N}$) respectively.\n",
    "> * Use \"frozen\" instances of the `scipy.stats.norm` continuous RV object to represent the prior PDF, likelihood function, and posterior PDF.  Strictly speaking, *the likelihood is not a PDF* (for $\\mu$), but since we usually only need the likelihood to be specified up to an arbitrary constant factor, it is computationally convenient to represent the likelihood with a PDF, so we can use SciPy's built-in functions to compute it.\n",
    "> * Use the `FigLRAxes` class (provided in the incomplete module) to plot the prior and posterior PDFs against the left axis, and the likelihood function against the right axis.\n",
    "> * Generate a figure resembling the one on the right of Lec07 slide 14.  The relevant parameter values are provided in the template `.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.3",
     "points=2"
    ]
   },
   "source": [
    "### Problem 3.3 (2 points):\n",
    "\n",
    "> * Write a second function, `plot_norm_norm2(mus, mu0, w0, dbar, w)`, that does the same computation and plotting *numerically*, along the lines of the binomial example in the Lab04 notebook, i.e., evaluating the prior and likelihood on a grid of $\\mu$ values, computing the marginal likelihood numerically with the trapezoid rule, and using those ingredients to compute the posterior PDF on a grid.\n",
    "> * Have this function return the grid of posterior PDF values.\n",
    "> * In the `norm_norm.py` file, write a test function to verify normalization of the computed posterior (a stub is provided). In your notebook, run the test using the `pytest` command."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
