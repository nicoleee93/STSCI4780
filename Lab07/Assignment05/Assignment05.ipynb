{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 05\n",
    "\n",
    "**Due:** 2022-03-17, 11:59 PM, as a Jupyter notebook (with related files) submitted via your repo in the course GitHub organization.  Edit the provided Solutions05 notebook with your solutions. Please **do not modify the subproblem cells** in your solutions notebook.\n",
    "\n",
    "*Note that Problem 2.3 is for 5780 students only.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Your assignment again has a `univariate_bayes.py` file that implements a `univariate_bayes` module with a single class, `UnivariateBayesianInference`.  This version is a revision of the one provided for Assignment 04; it has been refactored, and it includes some enhancements.  For the problems in this week's assignment, you will further enhance the capabilities of `UnivariateBayesianInference`.  Note that stubs (incomplete blocks of code) are provided to guide some of your work.\n",
    "\n",
    "Your repo also has a `poisson_binomial_cauchy.py` file, meant to be used as module, with three classes that subclass `UnivariateBayesianInference`:\n",
    "* `PoissonRateInference`\n",
    "* `BinomialInference`\n",
    "* `CauchyLocationInference`\n",
    "\n",
    "These classes appeared in the previous lab and assignment.\n",
    "\n",
    "As you modify the code, be sure to maintain docstrings:  revise the module docstring to note that you have revised the module's code, and provide docstrings for any new methods you write.  Comment your code appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Laplace approximation for *UnivariateBayesianInference*\n",
    "\n",
    "`UnivariateBayesianInference` has a stub method, `laplace`, for implementing Laplace's approximation to expectation integrals that arise in Bayesian inference.  For this problem, you'll complete the implementation, and demonstrate it on some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=1.1",
     "points=2"
    ]
   },
   "source": [
    "### Problem 1.1 (2 points):\n",
    "\n",
    "> Complete the implementation of the `laplace` method.  The stub locates the peak of the integrand and finds the amplitude at the peak; you'll need to calculate the curvature and use it to compute and return the values indicated in the method's docstring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=1.2",
     "points=3"
    ]
   },
   "source": [
    "### Problem 1.2 (3 points):\n",
    "\n",
    "Demonstrate the operation of the `laplace` method in the notebook by doing the following for one example each using the Poisson, binomial, and Cauchy inference objects.  Choose or simulate data of modest size (modest number of counts for Poisson & binomial; modest sample size for Cauchy), so the posterior PDFs are not too close to a normal PDF.\n",
    "\n",
    "> * Illustrate the Laplace approximation for computing the marginal likelihood:  Plot the *quasi*posterior as a solid line, and plot the Gaussian approximation of the quasiposterior corresponding to the Laplace approximation for the marginal likelihood integral as a dashed line.\n",
    "> * Display (print in the notebook, with annotations/explanations) values of the marginal likelihood and posterior mean calculated by quadrature and by the Laplace approximation.  Use Python's string formatting to show the results with appropriate precision (e.g., so 2 or 3 digits of variation between the results are shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Posterior sampling for `UnivariateBayesianInference`\n",
    "\n",
    "For this problem, you'll add IID posterior sampling capability to the `UnivariateBayesianInference` base class and use it for some inference problems.  The class already has a method, `samp_cdf`, implementing sampling via the inverse-CDF method; you'll add a new method implementing the accept/reject method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=2.1",
     "points=2"
    ]
   },
   "source": [
    "### Problem 2.1 (2 points):\n",
    "\n",
    "> Add a new method, with signature `sample_ar(self)`, that implements the accept/reject method for sampling from the posterior distribution.  Calling it (with no user-supplied arguments) should return a single sample from the posterior distribution.  Be sure to note quantities already computed in `__init__` that may be useful for your implementation.  In `__init__` a data attribute, `n_ar`, is initialized to 0.  Use this in your method to keep track of the total number of *candidate* samples `sample_ar` generates (i.e., the sum of rejected and accepted candidates).  You'll use this to estimate the efficiency of the sampler after repeated calls of `sample_ar`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=2.2",
     "points=3"
    ]
   },
   "source": [
    "### Problem 2.2 (3 points):\n",
    "\n",
    "Demonstrate the operation of the posterior sampling methods in the notebook by doing the following for one example each using the binomial and Cauchy inference objects.\n",
    "\n",
    "> * Plot the posterior as a solid line (with the `plot` method of an inference instance), and plot a normalized histogram of a few thousand posterior samples from `sample_cdf` on the same plot.  You may need to use the `alpha` parameter in the plotting commands so the curve and histogram can both be seen.\n",
    "> * In a new figure, do the same thing, but now use your new `sample_ar` method.\n",
    "> * Use the posterior samples from `sample_ar` to compute the posterior mean and standard deviation. Display the values computed by quadrature and by posterior sampling in the notebook.  Use Python's string formatting to show the results with appropriate precision.\n",
    "> * Display the sampling efficiency in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=2.3",
     "points=2",
     "5780"
    ]
   },
   "source": [
    "### Problem 2.3 (2 points, for 5780 students only):\n",
    "\n",
    "Find a **highest posterior density (HPD) credible region** for the Cauchy example:\n",
    "> * Compute the posterior density for every posterior sample you collected using `sample_ar`.\n",
    "> * Sort the values, and use the sorted values to find the densities bounding 50% and 90% HPD regions.\n",
    "> * On a new figure, plot the PDF as a solid curve, and plot horizontal dashed or dotted lines indicating the density levels that bound the two HPD regions (the boundaries for the regions are just where these lines cross the curve).  Use Matplotlib's `axhline` function to plot the lines, and `annotate` to label each line with the corresponding probability or percentage."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
